{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import machine learning libraries including scikit-learn, numpy, pandas, matplotlib, seaborn, and xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis - Missing values, data types, and descriptive stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset Airline Reviews and Ratings\n",
    "df = pd.read_csv('UCI_Heart_Disease_Dataset_Combined.csv')\n",
    "\n",
    "#change all variable names to snake_case\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# change name chestpaintype to chest_pain_type\n",
    "df.rename(columns={'chestpaintype': 'chest_pain_type'}, inplace=True)\n",
    "\n",
    "#change name restingecg to resting_ecg\n",
    "df.rename(columns={'restingecg': 'resting_ecg'}, inplace=True)\n",
    "\n",
    "#change name fastingbs to fasting_bs\n",
    "df.rename(columns={'fastingbs': 'fasting_bs'}, inplace=True)\n",
    "\n",
    "#change name restingecg to resting_ecg\n",
    "df.rename(columns={'restingecg': 'resting_ecg'}, inplace=True)\n",
    "\n",
    "#change name maxhr to max_hr\n",
    "df.rename(columns={'maxhr': 'max_hr'}, inplace=True)\n",
    "\n",
    "#change name exerciseangina to exercise_angina\n",
    "df.rename(columns={'exerciseangina': 'exercise_angina'}, inplace=True)\n",
    "\n",
    "#change name oldpeak to old_peak\n",
    "df.rename(columns={'oldpeak': 'old_peak'}, inplace=True)\n",
    "\n",
    "#change name heartdisease to heart_disease\n",
    "df.rename(columns={'heartdisease': 'heart_disease'}, inplace=True)\n",
    "\n",
    "#change name restingbp to resting_bp\n",
    "df.rename(columns={'restingbp': 'resting_bp'}, inplace=True)\n",
    "\n",
    "#print the first 5 rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "#do a summary of the dataset\n",
    "print(df.describe())\n",
    "\n",
    "#change all variable names to snake_case\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some basic Exploratory Data Analysis (EDA)\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check the data types of each column\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "print(df['heart_disease'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a histogram of all the variables\n",
    "df.hist(bins=20, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code generates a boxplot for the variables age, resting_bp, cholesterol, max_hr, and old_peak\n",
    "with respect to the target variable heart_disease. The boxplot provides a visual representation of the\n",
    "distribution of these variables for different values of heart_disease.\n",
    "\n",
    "Parameters:\n",
    "    - x: The target variable heart_disease.\n",
    "    - y: The variables age, resting_bp, cholesterol, max_hr, and old_peak.\n",
    "    - data: The dataframe containing the data.\n",
    "\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(2,3,1)\n",
    "sns.boxplot(x='heart_disease', y='age', data=df)\n",
    "plt.subplot(2,3,2)\n",
    "sns.boxplot(x='heart_disease', y='resting_bp', data=df)\n",
    "plt.subplot(2,3,3)\n",
    "sns.boxplot(x='heart_disease', y='cholesterol', data=df)\n",
    "plt.subplot(2,3,4)\n",
    "sns.boxplot(x='heart_disease', y='max_hr', data=df)\n",
    "plt.subplot(2,3,5)\n",
    "sns.boxplot(x='heart_disease', y='old_peak', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sex, chest_pain type, fasting_bs, resting_ecg, exercise_angina\n",
    "\n",
    "# make a barplot of sex vs heart_disease\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='sex', y='heart_disease', data=df)\n",
    "\n",
    "# make a barplot of chest_pain_type vs heart_disease\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='chest_pain_type', y='heart_disease', data=df)\n",
    "\n",
    "# make a barplot of fasting_bs vs heart_disease\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='fasting_bs', y='heart_disease', data=df)\n",
    "\n",
    "# make a barplot of resting_ecg vs heart_disease\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='resting_ecg', y='heart_disease', data=df)\n",
    "\n",
    "# make a barplot of exercise_angina vs heart_disease\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='exercise_angina', y='heart_disease', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a correlation matrix\n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print names of all columns\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One of the important measures of accuracy will be false positive rate. We write a function to calculate the false positive rate and a function to cross validate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that calulcates the false negative rate\n",
    "def calculate_fnr(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fn = cm[1][0]\n",
    "    tp = cm[1][1]\n",
    "    return fn / (fn + tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to run all the models on the same training data and finally test them using a test set that is not used in training. We split the data and keep the train set the same throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that stores the accuracy, RMSE, and false positive rate of the models\n",
    "results = {}\n",
    "results[\"Metric\"] = [\"Accuracy\", \"RMSE\", \"False Positive Rate\"]\n",
    "\n",
    "#split the data into the train set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('heart_disease', axis=1)\n",
    "y = df['heart_disease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "def calculate_fnr(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    fnr = fn / (fn + tn)\n",
    "    return fnr\n",
    "\n",
    "# Create a scorer using make_scorer\n",
    "fnr_scorer = make_scorer(calculate_fnr, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Baseline Models - Linear Regression and Naive regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the naive model only on training data, print the accuracy, RMSE, and false negative rate. do not store anything in the dictionary, do that later after cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the linear regression model only on training data, print the accuracy, RMSE, and false positive rate. do not store anything in the dictionary, do that later after cross validation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "linear = LinearRegression()\n",
    "linear.fit(X_train, y_train)\n",
    "y_pred = linear.predict(X_train)\n",
    "y_pred = np.round(y_pred)\n",
    "y_pred = np.clip(y_pred, 0, 1)\n",
    "print('Linear Regression Model')\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "\n",
    "# do cross validation on the linear regression model. put accuray and fpr as nan since these are not calculated for linear regression\n",
    "acc_linear = np.nan\n",
    "rmse_linear = cross_val_score(linear, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('Cross Validation RMSE:', -rmse_linear.mean())\n",
    "fp_linear = np.nan\n",
    "\n",
    "#store the results of the linear regression model in the dictionary\n",
    "results[\"Linear Regression\"] = [acc_linear, -rmse_linear, fp_linear]\n",
    "\n",
    "# make combination of polynomial transformation of the features to the 2nd degree and run all those combinations through the linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the random forest model only on training data, print the accuracy, RMSE, and false positive rate. do not store anything in the dictionary, do that later after cross validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred = random_forest.predict(X_train)\n",
    "print('Random Forest Model')\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_train, y_pred))\n",
    "\n",
    "# do cross validation on the random forest model.\n",
    "acc_random_forest = cross_val_score(random_forest, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross Validation Accuracy:', acc_random_forest.mean())\n",
    "rmse_random_forest = cross_val_score(random_forest, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('Cross Validation RMSE:', -rmse_random_forest.mean())\n",
    "fp_random_forest = calculate_mean_fpr(random_forest, X_train, y_train, 5)\n",
    "print('Cross Validation False Positive Rate:', fp_random_forest)\n",
    "\n",
    "#store the results of the random forest model in the dictionary\n",
    "results[\"Random Forest\"] = [acc_random_forest.mean(), -rmse_random_forest.mean(), fp_random_forest]\n",
    "\n",
    "# run a feature importance analysis on this model\n",
    "importances = random_forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    \n",
    "# Plot the feature importances of the random forest model\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "#suggest ways to improve the model through feature engineering\n",
    "#drop the least important features\n",
    "X_train = X_train.drop(['fasting_bs', 'resting_ecg'], axis=1)\n",
    "X_test = X_test.drop(['fasting_bs', 'resting_ecg'], axis=1)\n",
    "\n",
    "#run the random forest model again\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred = random_forest.predict(X_train)\n",
    "print('Random Forest Model')\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the gradient boosting model only on training data, print the accuracy, RMSE, and false positive rate. do not store anything in the dictionary, do that later after cross validation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=42)\n",
    "gradient_boosting.fit(X_train, y_train)\n",
    "y_pred = gradient_boosting.predict(X_train)\n",
    "print('Gradient Boosting Model')\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_train, y_pred))\n",
    "\n",
    "# do cross validation on the gradient boosting model.\n",
    "acc_gradient_boosting = cross_val_score(gradient_boosting, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross Validation Accuracy:', acc_gradient_boosting.mean())\n",
    "rmse_gradient_boosting = cross_val_score(gradient_boosting, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('Cross Validation RMSE:', -rmse_gradient_boosting.mean())\n",
    "fp_gradient_boosting = calculate_mean_fpr(gradient_boosting, X_train, y_train, 5)\n",
    "print('Cross Validation False Positive Rate:', fp_gradient_boosting)\n",
    "\n",
    "#store the results of the gradient boosting model in the dictionary\n",
    "results[\"Gradient Boosting\"] = [acc_gradient_boosting.mean(), -rmse_gradient_boosting.mean(), fp_gradient_boosting]\n",
    "\n",
    "#store the results of the models in a dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run xgboost model only on training data, print the accuracy, RMSE, and false positive rate. do not store anything in the dictionary, do that later after cross validation\n",
    "# run a xgboost model on training data, print the accuracy, RMSE, and false positive rate. do not store anything in the dictionary, do that later after cross validation\n",
    "from xgboost import XGBClassifier\n",
    "xgboost = XGBClassifier(random_state=42)\n",
    "xgboost.fit(X_train, y_train)\n",
    "y_pred = xgboost.predict(X_train)\n",
    "print('XGBoost Model')\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_train, y_pred))\n",
    "\n",
    "# do cross validation on the xgboost model.\n",
    "acc_xgboost = cross_val_score(xgboost, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross Validation Accuracy:', acc_xgboost.mean())\n",
    "rmse_xgboost = cross_val_score(xgboost, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('Cross Validation RMSE:', -rmse_xgboost.mean())\n",
    "fp_xgboost = calculate_mean_fpr(xgboost, X_train, y_train, 5)\n",
    "print('Cross Validation False Positive Rate:', fp_xgboost)\n",
    "\n",
    "# store the results in the dictionary\n",
    "results['XGBoost'] = [acc_xgboost.mean(), -rmse_xgboost.mean(), fp_xgboost]\n",
    "\n",
    "#show the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector MAchine (SVM) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the support vector machine model only on training data, print the accuracy, RMSE, and false positive rate. do not store anything in the dictionary, do that later after cross validation\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_train)\n",
    "print('Support Vector Machine Model')\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train, y_pred).ravel()\n",
    "\n",
    "# Calculate false positive rate on training data\n",
    "fpr_train = fp_train / (fp_train + tn_train)\n",
    "print('FPR:', fpr_train)\n",
    "\n",
    "# do cross validation on the support vector machine model.\n",
    "acc_svm = cross_val_score(svm, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross Validation Accuracy:', acc_svm.mean())\n",
    "rmse_svm = cross_val_score(svm, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('Cross Validation RMSE:', -rmse_svm.mean())\n",
    "fp_svm = cross_val_score(svm, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "print('Cross Validation False Positive Rate:', 1-fp_svm.mean())\n",
    "\n",
    "# store the results in the dictionary\n",
    "results['Support Vector Machine'] = [acc_svm.mean(), -rmse_svm.mean(), 1-fp_svm.mean()]\n",
    "\n",
    "#show the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run decision tree model only on training data, print the accuracy, RMSE, and false positive rate. do not store anything in the dictionary, do that later after cross validation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision = DecisionTreeClassifier()\n",
    "decision.fit(X_train, y_train)\n",
    "y_pred = decision.predict(X_train)\n",
    "print('Decision Tree Model')\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train, y_pred).ravel()\n",
    "\n",
    "# Calculate false positive rate on training data\n",
    "fpr_train = fp_train / (fp_train + tn_train)\n",
    "print('FPR:', fpr_train)\n",
    "\n",
    "# do cross validation on the decision tree model.\n",
    "acc_decision = cross_val_score(decision, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross Validation Accuracy:', acc_decision.mean())\n",
    "rmse_decision = cross_val_score(decision, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('Cross Validation RMSE:', -rmse_decision.mean())\n",
    "fp_decision = cross_val_score(decision, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "print('Cross Validation False Positive Rate:', 1-fp_decision.mean())\n",
    "\n",
    "# store the results in the dictionary\n",
    "results['Decision Tree'] = [acc_decision.mean(), -rmse_decision.mean(), 1-fp_decision.mean()]\n",
    "\n",
    "#show the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a knn model on multiple ks, plotting the number of neighbors vs the accuracy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "accs = []\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    acc = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    accs.append(acc)\n",
    "plt.plot(range(1, 21), accs)\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Number of Neighbors vs Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# from the plot, we see that 7 neighbors is the best number of neighbors. We will run the knn model with 7 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_train)\n",
    "print('KNN Model')\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_train, y_pred))\n",
    "\n",
    "# do cross validation on the knn model.\n",
    "acc_knn = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross Validation Accuracy:', acc_knn.mean())\n",
    "rmse_knn = cross_val_score(knn, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('Cross Validation RMSE:', -rmse_knn.mean())\n",
    "fp_knn = calculate_mean_fpr(knn, X_train, y_train, 5)\n",
    "print('Cross Validation False Positive Rate:', fp_knn)\n",
    "\n",
    "# store the results in the dictionary\n",
    "results['KNN'] = [acc_knn.mean(), -rmse_knn.mean(), fp_knn]\n",
    "\n",
    "#show the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a logistic regression model on training data, print the accuracy, RMSE, and false positive rate. do not store anything in the dictionary, do that later after cross validation\n",
    "# increase the max_iter to 1000\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression(max_iter=1000)\n",
    "logistic.fit(X_train, y_train)\n",
    "y_pred = logistic.predict(X_train)\n",
    "print('Logistic Regression Model')\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_train, y_pred))\n",
    "\n",
    "# do cross validation on the logistic regression model.\n",
    "acc_logistic = cross_val_score(logistic, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross Validation Accuracy:', acc_logistic.mean())\n",
    "rmse_logistic = cross_val_score(logistic, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('Cross Validation RMSE:', -rmse_logistic.mean())\n",
    "fp_logistic = calculate_mean_fpr(logistic, X_train, y_train, 5)\n",
    "print('Cross Validation False Positive Rate:', fp_logistic)\n",
    "\n",
    "# store the results in the dictionary\n",
    "results['Logistic Regression'] = [acc_logistic.mean(), -rmse_logistic.mean(), fp_logistic]\n",
    "\n",
    "#show the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a neural network model on training data, print the accuracy, RMSE, and false positive rate. do not store anything in the dictionary, do that later after cross validation\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "neural = MLPClassifier(random_state=42)\n",
    "neural.fit(X_train, y_train)\n",
    "y_pred = neural.predict(X_train)\n",
    "print('Neural Network Model')\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_train, y_pred))\n",
    "\n",
    "# do cross validation on the neural network model.\n",
    "acc_neural = cross_val_score(neural, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross Validation Accuracy:', acc_neural.mean())\n",
    "rmse_neural = cross_val_score(neural, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('Cross Validation RMSE:', -rmse_neural.mean())\n",
    "fp_neural = calculate_mean_fpr(neural, X_train, y_train, 5)\n",
    "print('Cross Validation False Positive Rate:', fp_neural)\n",
    "\n",
    "# store the results in the dictionary\n",
    "results['Neural Network'] = [acc_neural.mean(), -rmse_neural.mean(), fp_neural]\n",
    "\n",
    "#show the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a naive bayes model on training data, print the accuracy, RMSE, and false positive rate. do not store anything in the dictionary, do that later after cross validation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "y_pred = naive_bayes.predict(X_train)\n",
    "print('Naive Bayes Model')\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_train, y_pred))\n",
    "\n",
    "# do cross validation on the naive bayes model.\n",
    "acc_naive_bayes = cross_val_score(naive_bayes, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross Validation Accuracy:', acc_naive_bayes.mean())\n",
    "rmse_naive_bayes = cross_val_score(naive_bayes, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('Cross Validation RMSE:', -rmse_naive_bayes.mean())\n",
    "fp_naive_bayes = calculate_mean_fpr(naive_bayes, X_train, y_train, 5)\n",
    "print('Cross Validation False Positive Rate:', fp_naive_bayes)\n",
    "\n",
    "# store the results in the dictionary\n",
    "results['Naive Bayes'] = [acc_naive_bayes.mean(), -rmse_naive_bayes.mean(), fp_naive_bayes]\n",
    "\n",
    "#show the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the metrics based on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Set 'Metric' column as index\n",
    "results_df.set_index('Metric', inplace=True)\n",
    "\n",
    "# Transpose the DataFrame\n",
    "results_df = results_df.T\n",
    "\n",
    "# Drop NaN values\n",
    "results_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy of different models\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=results_df.index, y=results_df['Accuracy'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Different Models')\n",
    "plt.tight_layout()\n",
    "\n",
    "# labeling the bars with the accuracy values\n",
    "for i in range(len(results_df)):\n",
    "    plt.text(i, results_df['Accuracy'][i], round(results_df['Accuracy'][i], 2), ha='center', va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the RMSE of different models\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=results_df.index, y=results_df['RMSE'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE of Different Models')\n",
    "plt.tight_layout()\n",
    "\n",
    "# labeling the bars with the RMSE values\n",
    "for i in range(len(results_df)):\n",
    "    plt.text(i, results_df['RMSE'][i], round(results_df['RMSE'][i], 2), ha='center', va='bottom')\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the False Positive Rate of different models\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=results_df.index, y=results_df['False Positive Rate'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.title('False Positive Rate of Different Models')\n",
    "plt.tight_layout()\n",
    "\n",
    "# labeling the bars with the False Positive Rate values\n",
    "for i in range(len(results_df)):\n",
    "    plt.text(i, results_df['False Positive Rate'][i], round(results_df['False Positive Rate'][i], 2), ha='center', va='bottom')\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results dataframe\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the models on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The test data was left untouched uptil now. Running the numbers again on the test data will validate our findings about which model is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run all the models on the test data and store the results in a dictionary\n",
    "# run the naive model on the test data\n",
    "y_pred = naive.predict(X_test)\n",
    "print('Naive Model')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_test, y_pred))\n",
    "\n",
    "# store the results in the dictionary\n",
    "results_test = {}\n",
    "results_test[\"Metric\"] = [\"Accuracy\", \"RMSE\", \"False Positive Rate\"]\n",
    "results_test[\"Naive\"] = [accuracy_score(y_test, y_pred), mean_squared_error(y_test, y_pred), calculate_fpr(y_test, y_pred)]\n",
    "\n",
    "# run the linear regression model on the test data\n",
    "y_pred = linear.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "y_pred = np.clip(y_pred, 0, 1)\n",
    "print('Linear Regression Model')\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# store the results in the dictionary\n",
    "results_test[\"Linear Regression\"] = [np.nan, mean_squared_error(y_test, y_pred), np.nan]\n",
    "\n",
    "# run the random forest model on the test data\n",
    "y_pred = random_forest.predict(X_test)\n",
    "print('Random Forest Model')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_test, y_pred))\n",
    "\n",
    "# store the results in the dictionary\n",
    "results_test[\"Random Forest\"] = [accuracy_score(y_test, y_pred), mean_squared_error(y_test, y_pred), calculate_fpr(y_test, y_pred)]\n",
    "\n",
    "# run the gradient boosting model on the test data\n",
    "y_pred = gradient_boosting.predict(X_test)\n",
    "print('Gradient Boosting Model')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_test, y_pred))\n",
    "\n",
    "# store the results in the dictionary\n",
    "results_test[\"Gradient Boosting\"] = [accuracy_score(y_test, y_pred), mean_squared_error(y_test, y_pred), calculate_fpr(y_test, y_pred)]\n",
    "\n",
    "# run the xgboost model on the test data\n",
    "y_pred = xgboost.predict(X_test)\n",
    "print('XGBoost Model')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_test, y_pred))\n",
    "\n",
    "# store the results in the dictionary\n",
    "results_test[\"XGBoost\"] = [accuracy_score(y_test, y_pred), mean_squared_error(y_test, y_pred), calculate_fpr(y_test, y_pred)]\n",
    "\n",
    "# run the support vector machine model on the test data\n",
    "y_pred = svm.predict(X_test)\n",
    "print('Support Vector Machine Model')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_test, y_pred))\n",
    "\n",
    "# store the results in the dictionary\n",
    "results_test[\"Support Vector Machine\"] = [accuracy_score(y_test, y_pred), mean_squared_error(y_test, y_pred), calculate_fpr(y_test, y_pred)]\n",
    "\n",
    "# run the decision tree model on the test data\n",
    "y_pred = decision.predict(X_test)\n",
    "print('Decision Tree Model')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_test, y_pred))\n",
    "\n",
    "# run the decision tree model on the test data\n",
    "results_test[\"Decision Tree\"] = [accuracy_score(y_test, y_pred), mean_squared_error(y_test, y_pred), calculate_fpr(y_test, y_pred)]\n",
    "\n",
    "# run the knn model on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "print('KNN Model')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_test, y_pred))\n",
    "\n",
    "# store the results in the dictionary\n",
    "results_test[\"KNN\"] = [accuracy_score(y_test, y_pred), mean_squared_error(y_test, y_pred), calculate_fpr(y_test, y_pred)]\n",
    "\n",
    "# run the logistic regression model on the test data\n",
    "y_pred = logistic.predict(X_test)\n",
    "print('Logistic Regression Model')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_test, y_pred))\n",
    "\n",
    "# store the results in the dictionary\n",
    "results_test[\"Logistic Regression\"] = [accuracy_score(y_test, y_pred), mean_squared_error(y_test, y_pred), calculate_fpr(y_test, y_pred)]\n",
    "\n",
    "# run the neural network model on the test data\n",
    "y_pred = neural.predict(X_test)\n",
    "print('Neural Network Model')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_test, y_pred))\n",
    "\n",
    "# store the results in the dictionary\n",
    "results_test[\"Neural Network\"] = [accuracy_score(y_test, y_pred), mean_squared_error(y_test, y_pred), calculate_fpr(y_test, y_pred)]\n",
    "\n",
    "# run the naive bayes model on the test data\n",
    "y_pred = naive_bayes.predict(X_test)\n",
    "print('Naive Bayes Model')\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_test, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_test, y_pred))\n",
    "\n",
    "# store the results in the dictionary\n",
    "results_test[\"Naive Bayes\"] = [accuracy_score(y_test, y_pred), mean_squared_error(y_test, y_pred), calculate_fpr(y_test, y_pred)]\n",
    "\n",
    "# print the results dictionary\n",
    "print(results_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the test data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the results dictionary to a dataframe\n",
    "results_test_df = pd.DataFrame(results_test)\n",
    "\n",
    "# Set 'Metric' column as index\n",
    "results_test_df.set_index('Metric', inplace=True)\n",
    "\n",
    "# Transpose the DataFrame\n",
    "results_test_df = results_test_df.T\n",
    "\n",
    "# Drop NaN values\n",
    "results_test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotthe accuracy of different models on the test data\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=results_test_df.index, y=results_test_df['Accuracy'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Different Models on Test Data')\n",
    "plt.tight_layout()\n",
    "\n",
    "# labeling the bars with the accuracy values\n",
    "for i in range(len(results_test_df)):\n",
    "    plt.text(i, results_test_df['Accuracy'][i], round(results_test_df['Accuracy'][i], 2), ha='center', va='bottom')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# plot the RMSE of different models on the test data\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=results_test_df.index, y=results_test_df['RMSE'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE of Different Models on Test Data')\n",
    "plt.tight_layout()\n",
    "\n",
    "# labeling the bars with the RMSE values\n",
    "for i in range(len(results_test_df)):\n",
    "    plt.text(i, results_test_df['RMSE'][i], round(results_test_df['RMSE'][i], 2), ha='center', va='bottom')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# plot the False Positive Rate of different models on the test data\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=results_test_df.index, y=results_test_df['False Positive Rate'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.title('False Positive Rate of Different Models on Test Data')\n",
    "plt.tight_layout()\n",
    "\n",
    "# labeling the bars with the False Positive Rate values\n",
    "for i in range(len(results_test_df)):\n",
    "    plt.text(i, results_test_df['False Positive Rate'][i], round(results_test_df['False Positive Rate'][i], 2), ha='center', va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do these results generalise on other datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Heart_Disease_Prediction.csv')\n",
    "\n",
    "#head of the dataset\n",
    "print(df2.head())\n",
    "\n",
    "# what are the unique values in the columnn heart disease\n",
    "print(df2['Heart Disease'].unique())\n",
    "\n",
    "# show the data types of each column\n",
    "print(df2.dtypes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the column Heart Disease is an object, we need to convert it to string\n",
    "df2['Heart Disease'] = df2['Heart Disease'].astype(str)\n",
    "\n",
    "# change Hweart Disease to binary variable with Presence as 1 and Absence as 0\n",
    "df2['Heart Disease'] = df2['Heart Disease'].replace({'Presence': 1, 'Absence': 0})\n",
    "\n",
    "#head of the dataset\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run random forest, xgboost, gradient boosting, and decision tree models on the new dataset\n",
    "\n",
    "#first create a dictionary to store the results\n",
    "results_new = {}\n",
    "results_new[\"Metric\"] = [\"Accuracy\", \"RMSE\", \"False Positive Rate\"]\n",
    "\n",
    "#split the data into the train set and test set\n",
    "X = df2.drop('Heart Disease', axis=1)\n",
    "y = df2['Heart Disease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# run the random forest model on the new dataset\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred = random_forest.predict(X_train)\n",
    "print('Random Forest Model')\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_train, y_pred))\n",
    "\n",
    "# do cross validation on the random forest model.\n",
    "acc_random_forest = cross_val_score(random_forest, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross Validation Accuracy:', acc_random_forest.mean())\n",
    "rmse_random_forest = cross_val_score(random_forest, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('Cross Validation RMSE:', -rmse_random_forest.mean())\n",
    "fp_random_forest = calculate_mean_fpr(random_forest, X_train, y_train, 5)\n",
    "print('Cross Validation False Positive Rate:', fp_random_forest)\n",
    "\n",
    "#store the results of the random forest model in the dictionary\n",
    "results_new[\"Random Forest\"] = [acc_random_forest.mean(), -rmse_random_forest.mean(), fp_random_forest]\n",
    "\n",
    "# run the gradient boosting model on the new dataset\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=42)\n",
    "gradient_boosting.fit(X_train, y_train)\n",
    "y_pred = gradient_boosting.predict(X_train)\n",
    "print('Gradient Boosting Model')\n",
    "print('Accuracy:', accuracy_score(y_train, y_pred))\n",
    "print('RMSE:', mean_squared_error(y_train, y_pred))\n",
    "print('False Positive Rate:', calculate_fpr(y_train, y_pred))\n",
    "\n",
    "# do cross validation on the gradient boosting model.\n",
    "acc_gradient_boosting = cross_val_score(gradient_boosting, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Cross Validation Accuracy:', acc_gradient_boosting.mean())\n",
    "rmse_gradient_boosting = cross_val_score(gradient_boosting, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print('Cross Validation RMSE:', -rmse_gradient_boosting.mean())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
